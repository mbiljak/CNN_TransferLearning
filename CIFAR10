import tensorflow as tf
from tensorflow.keras import layers, models, applications
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
import numpy as np
import time
import pandas as pd
import gc

# ---------------------- GPU Configuration ----------------------
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    print("\nGPU detected:")
    for device in physical_devices:
        print(f"- {device}")
    try:
        for device in physical_devices:
            tf.config.experimental.set_memory_growth(device, True)
        print("Memory growth enabled")
    except Exception as e:
        print(f"Memory growth error: {e}")
else:
    print("\nGPU not detected - using CPU")

tf.get_logger().setLevel('ERROR')
tf.debugging.set_log_device_placement(False)

# ----------------------------------------------------------------

start_time = time.time()

# Load CIFAR-10 dataset
print("Loading CIFAR-10 dataset...")
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

TRAIN_SAMPLES = 45000 #45000
TEST_SAMPLES = 8000 #8000
indices_train = np.random.choice(len(x_train), TRAIN_SAMPLES, replace=False)
indices_test = np.random.choice(len(x_test), TEST_SAMPLES, replace=False)
x_train = x_train[indices_train]
y_train = y_train[indices_train]
x_test = x_test[indices_test]
y_test = y_test[indices_test]

print(f"Using {len(x_train)} training samples and {len(x_test)} test samples")

x_train_norm = x_train.astype('float32') 
x_test_norm = x_test.astype('float32') 

y_train_cat = tf.keras.utils.to_categorical(y_train, 10)
y_test_cat = tf.keras.utils.to_categorical(y_test, 10)

data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
])

def create_efficient_cnn():
    model = models.Sequential([
        layers.InputLayer(input_shape=(32, 32, 3)),
        layers.Conv2D(16, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ])
    return model

model_scratch = create_efficient_cnn()
model_scratch.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

BATCH_SIZE = 64

print("\nTraining custom CNN...")
history_scratch = model_scratch.fit(
    x_train_norm, y_train_cat,
    epochs=15,
    batch_size=BATCH_SIZE,
    validation_data=(x_test_norm, y_test_cat),
    callbacks=callbacks,
    verbose=1
)

def create_feature_extractor():
    """Create feature extractor from trained CNN"""
    # Create a new input layer with the same shape as the original model
    input_shape = (32, 32, 3)  # CIFAR-10 image dimensions
    inputs = tf.keras.Input(shape=input_shape)
    
    # Create a new model that processes inputs through the first 8 layers of model_scratch
    x = inputs
    # Apply the first 8 layers (up to and including the last Conv2D layer)
    # Sequential model layers are zero-indexed, so we use layers 0-7
    for i in range(8):
        x = model_scratch.layers[i](x)
    
    # Create and return the feature extractor model
    return tf.keras.Model(inputs=inputs, outputs=x)

feature_extractor = create_feature_extractor()
print("Feature extractor summary:")
feature_extractor.summary()

# Extract features in batches (keep the rest of the code the same)
def extract_features_in_batches(model, data, batch_size=64):
    features_list = []
    num_samples = len(data)
    for i in range(0, num_samples, batch_size):
        batch = data[i:i+batch_size]
        features = model.predict(batch, verbose=0)
        features_list.append(features)
    return np.concatenate(features_list, axis=0).reshape(num_samples, -1)

print("\nExtracting CNN features for SVM...")
x_train_features = extract_features_in_batches(feature_extractor, x_train_norm)
x_test_features = extract_features_in_batches(feature_extractor, x_test_norm)

# Apply PCA
print("Applying PCA...")
pca = PCA(n_components=100)
x_train_pca = pca.fit_transform(x_train_features)
x_test_pca = pca.transform(x_test_features)

# Train SVM
print("Training SVM...")
svm = SVC(kernel='rbf', C=1, gamma='scale')
svm.fit(x_train_pca, y_train.flatten())

# Evaluate SVM
svm_accuracy = accuracy_score(y_test.flatten(), svm.predict(x_test_pca))
print(f"SVM Accuracy: {svm_accuracy:.4f}")

# Cleanup features
del x_train_features, x_test_features, x_train_pca, x_test_pca
gc.collect()

# ---------------------- Transfer Learning with ResNet50 ----------------------
print("\nSetting up transfer learning with ResNet50...")
base_model = applications.ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(128, 128, 3)
)
base_model.trainable = False

inputs = layers.Input(shape=(32, 32, 3))
x = data_augmentation(inputs)

x = layers.Lambda(lambda img: tf.image.resize(img, (128, 128)))(x) #ResNet's asks for 244x244, but it's too computationally expensive

x = applications.resnet50.preprocess_input(x)
x = base_model(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation='relu')(x)
outputs = layers.Dense(10, activation='softmax')(x)

model_transfer = tf.keras.Model(inputs, outputs)

model_transfer.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("Training transfer learning model (frozen)...")
history_transfer_frozen = model_transfer.fit(
    x_train_norm, y_train_cat,
    epochs=5,
    batch_size=BATCH_SIZE,
    validation_data=(x_test_norm, y_test_cat),
    callbacks=callbacks,
    verbose=1
)

# Fine-tuning
base_model.trainable = True
for layer in base_model.layers[:-30]:
    layer.trainable = False

model_transfer.compile(
    optimizer=tf.keras.optimizers.Adam(0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("Fine-tuning transfer learning model...")
history_transfer = model_transfer.fit(
    x_train_norm, y_train_cat,
    epochs=5,
    batch_size=BATCH_SIZE,
    validation_data=(x_test_norm, y_test_cat),
    callbacks=callbacks,
    verbose=1
)

def combine_histories(h1, h2):
    return {
        'accuracy': h1.history['accuracy'] + h2.history['accuracy'],
        'val_accuracy': h1.history['val_accuracy'] + h2.history['val_accuracy'],
        'loss': h1.history['loss'] + h2.history['loss'],
        'val_loss': h1.history['val_loss'] + h2.history['val_loss']
    }

combined_history_transfer = combine_histories(history_transfer_frozen, history_transfer)

elapsed_time = time.time() - start_time
print(f"\nTotal runtime: {elapsed_time/60:.2f} minutes")

# Visualization
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_scratch.history['accuracy'], label='CNN Training')
plt.plot(history_scratch.history['val_accuracy'], label='CNN Testing')
plt.plot(combined_history_transfer['accuracy'], label='ResNet50 Training')
plt.plot(combined_history_transfer['val_accuracy'], label='ResNet50 Training')
plt.title('Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_scratch.history['loss'], label='CNN Training')
plt.plot(history_scratch.history['val_loss'], label='CNN Testing')
plt.plot(combined_history_transfer['loss'], label='ResNet50 Training')
plt.plot(combined_history_transfer['val_loss'], label='ResNet50 Testing')
plt.title('Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('training_comparison.png')
plt.show()

# Performance summary
summary_data = {
    'Model': ['Custom CNN', 'ResNet50 Transfer', 'CNN+SVM'],
    'Accuracy': [
        history_scratch.history['val_accuracy'][-1],
        combined_history_transfer['val_accuracy'][-1],
        svm_accuracy
    ]
}
summary_df = pd.DataFrame(summary_data)
print("\nPerformance Summary:")
print(summary_df.to_markdown(index=False))

print("\n=== Memory Optimization Summary ===")
print("1. GPU memory management enabled")
print("2. Dataset size reduced by 50%")
print("3. Model architecture simplified")
print("4. Batch processing used for feature extraction")
print("5. Explicit memory cleanup added")
print("6. ResNet50 used for transfer learning")
